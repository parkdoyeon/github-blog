{"componentChunkName":"component---src-templates-post-js","path":"/deep-learning-from-scratch-2-신경망-학습","result":{"data":{"markdownRemark":{"html":"<h3 id=\"개요\"><a href=\"#%EA%B0%9C%EC%9A%94\" aria-label=\"개요 permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>개요</h3>\n<ul>\n<li>'학습'이란 훈련 데이터로부터 가중치 매개변수의 최적값을 <strong>자동</strong>으로 획득하는 것을 뜻한다.</li>\n<li>\n<p>매개변수 최적값을 학습할 수 있도록 해주는 <strong>지표</strong>는 손실함수이다.</p>\n<ul>\n<li>\n<p>왜 정확도가 아닌 손실함수인가?</p>\n<ul>\n<li>정확도를 지표료 하면 매개변수의 미분이 대부분의 장소에서 0이 되기 때문이다. (= 소숫점 단위로 표현되는 연속적인 수치값이 아닌 단절된 숫자값으로 표현되기가 쉽다.)</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>손실함수를 최대한 작게 만들어주도록 하는 기법중 하나로, 함수의 기울기를 활용하는 것이 '경사법'이다.</li>\n<li>입력부터 출력까지 사람의 개입이 없다는 의미에서, 딥러닝을 종단간 기계학습(end-to-end learning)이라고 한다.</li>\n<li>한 데이터셋에만 지나치게 최적화된 상태를 <strong>오버피팅</strong>이라고 한다.</li>\n</ul>\n<h3 id=\"1-손실함수\"><a href=\"#1-%EC%86%90%EC%8B%A4%ED%95%A8%EC%88%98\" aria-label=\"1 손실함수 permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. 손실함수</h3>\n<h4 id=\"11-평균-제곱-오차\"><a href=\"#11-%ED%8F%89%EA%B7%A0-%EC%A0%9C%EA%B3%B1-%EC%98%A4%EC%B0%A8\" aria-label=\"11 평균 제곱 오차 permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1.1 평균 제곱 오차</h4>\n<p>$$ E = { { 1\\over2 } \\sum_ { k } ( y<em>k-t</em>k)^2 } $$</p>\n<ul>\n<li>\\(y<em>k\\)는 신경망이 추정한 출력값, \\(t</em>k\\)는 실제 정답레이블 값(원-핫 인코딩), k는 차원의 수를 의미한다.</li>\n<li>결과값으로 나타난 손실값의 합들이 높을수록 정답과 멀어지고, 적을수록 정답에 가깝다.</li>\n</ul>\n<h4 id=\"12-교차-엔트로피-오차\"><a href=\"#12-%EA%B5%90%EC%B0%A8-%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC-%EC%98%A4%EC%B0%A8\" aria-label=\"12 교차 엔트로피 오차 permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1.2 교차 엔트로피 오차</h4>\n<p>$$  E = - { \\sum_ { k } t<em>k logy</em>k } $$</p>\n<ul>\n<li>여기서 \\(logy\\)는 자연로그 \\(log_en\\)를 취한다.</li>\n<li>\n<p>\\(t<em>k\\)는 원-핫 인코딩 값이므로 실질적으로 \\(t</em>k\\)가 1일때의 \\(logy_k\\)을 계산한 값, 즉 정답 추정값의 자연로그를 계산하는 식이 된다.</p>\n<ul>\n<li>여기서 정답과 거리가 먼 결과가 발생할수록(x가 1에 가까워질수록) 엔트로피 오차(loss)가 더 크게 발생한다.</li>\n<li>\\(log<em>ex=y\\)의 그래프\n![log</em>ex=y의 그래프](/image/ml/2019-06-02-deep2-1.png)</li>\n<li>가령 신경망 출력이 0.6일때 교차 엔트로피 오차가 \\(-log0.6 = 0.51\\)이라면, 0.1일때는 \\(-log0.1=2.3\\)이 된다.</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"13-미니-배치-함수\"><a href=\"#13-%EB%AF%B8%EB%8B%88-%EB%B0%B0%EC%B9%98-%ED%95%A8%EC%88%98\" aria-label=\"13 미니 배치 함수 permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1.3 미니 배치 함수</h4>\n<p>$$  E = - { 1 \\over N } \\sum { \\sum_ { k } t<em>k logy</em>k } $$</p>\n<ul>\n<li>앞서서 구한 교차 엔트로피 오차를 모두 구해서 갯수만큼 나누면 평균 교차 엔트로피값을 구할 수 있다. 이렇게 하면 훈련 데이터 개수와 관계없이 통일된 지표를 구할 수 있다.</li>\n<li>전체 데이터가 수천 수십만개가 되면 교차 엔트로피 값을 구하는것은 무리가 있으므로 데이터 일부를 추려서 근사치로 이용할 수 있다.</li>\n<li>추려진 일부 데이터를 <strong>미니배치</strong>라고 한다.</li>\n</ul>\n<h3 id=\"2-수치미분\"><a href=\"#2-%EC%88%98%EC%B9%98%EB%AF%B8%EB%B6%84\" aria-label=\"2 수치미분 permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. 수치미분</h3>\n<h4 id=\"21-수치미분과-오차\"><a href=\"#21-%EC%88%98%EC%B9%98%EB%AF%B8%EB%B6%84%EA%B3%BC-%EC%98%A4%EC%B0%A8\" aria-label=\"21 수치미분과 오차 permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2.1 수치미분과 오차</h4>\n<ul>\n<li>\n<p>미분방법에는 수치 미분과 해석적 미분이 있다.</p>\n<ol>\n<li>수치미분은 실제 변화량을 수치적으로 계산하는 것으로, 변화값인 h에 최대한 작은 값(보통 \\(\\lim_{h\\to0}\\)으로 표현된다.)을 대입하여 계산한다.</li>\n<li>해석적 미분은 수식적으로 미분함수를 만들어 미분값을 찾는 것을 의미한다.</li>\n</ol>\n</li>\n<li>\n<p>수치미분을 코드로직에 적용할 때 h의 최솟값을 대입하다보면 아래와 같은 문제가 발생한다.\n수치미분을 구하는 파이썬 함수를 예로들면,</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">numerical_diff</span><span class=\"token punctuation\">(</span>f<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    h <span class=\"token operator\">=</span> <span class=\"token number\">10e</span><span class=\"token operator\">-</span><span class=\"token number\">50</span>\n    <span class=\"token keyword\">return</span> <span class=\"token punctuation\">(</span>f<span class=\"token punctuation\">(</span>x<span class=\"token operator\">+</span>h<span class=\"token punctuation\">)</span><span class=\"token operator\">+</span>f<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span><span class=\"token operator\">/</span>h<span class=\"token punctuation\">)</span></code></pre></div>\n<p>아래와 같이 소숫점 8자리 이하부터 생략해 최종 계산값에 오차가 발생한다.</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token operator\">>></span><span class=\"token operator\">></span> np.float32<span class=\"token punctuation\">(</span>1e-50<span class=\"token punctuation\">)</span>\n<span class=\"token number\">0.0</span></code></pre></div>\n<p>때문에 \\(10^{-4}\\)정도의 값을 사용할 것을 권장한다.</p>\n</li>\n<li>수치미분에서 발생하는 오차는 필연적이기 때문에, 변화량이 +h인 미분과 -h인 미분의 중앙값을 구하기도 한다.</li>\n</ul>\n<h4 id=\"22-편미분\"><a href=\"#22-%ED%8E%B8%EB%AF%B8%EB%B6%84\" aria-label=\"22 편미분 permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2.2 편미분</h4>\n<ul>\n<li>편미분은 변수가 2개 이상인 경우에 사용한다. 3차원으로 그래프가 그려지므로, 미분값을 구할때는 변수 하나에 초점을 맞추고 다른 변수는 값을 고정한다.</li>\n<li>편미분한 기울기 값에 마이너스 부호를 붙이고 2차원 벡터로 표현하면 각 지점에서 낮아지는 방향을 가리킨다. </li>\n<li>이말인 즉, <strong>기울기가 가리키는 곳은 각 장소의 함수에서 함수의 출력 값을 가장 크게 줄이는 방향</strong>이다.</li>\n</ul>\n<h4 id=\"23-경사법\"><a href=\"#23-%EA%B2%BD%EC%82%AC%EB%B2%95\" aria-label=\"23 경사법 permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2.3 경사법</h4>\n<ul>\n<li>경사법은 경사 하강법이라고 한다. 손실함수가 최소가 되는 값을 찾기위해 기울기를 활용하는 방법이다.</li>\n<li>기울기가 가리키는 곳에 정말 함수의 최솟값이 있는지는 보장할 수 없다. 실제로 최솟값이 없는 경우가 대부분이다.</li>\n<li>다만 그 방향으로 가야 줄일 수 있다는 사실은 확실하므로, 기울어진 방향으로 일정 거리만큼 이동해나가면서 줄이는게 경사법이라고 한다.</li>\n<li>최솟값을 찾으면 경사 하강법, 최댓값을 찾으면 경사 상승법이다. 단순히 기울기에 마이너스를 붙이냐 아니냐의 차이이므로 방법과 내용의 실질적인 차이는 없다.</li>\n</ul>\n<h3 id=\"231-경사법의-수식\"><a href=\"#231-%EA%B2%BD%EC%82%AC%EB%B2%95%EC%9D%98-%EC%88%98%EC%8B%9D\" aria-label=\"231 경사법의 수식 permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2.3.1 경사법의 수식</h3>\n<p>$$ x<em>0 = x</em>0 - \\eta { \\partial f \\over \\partial x<em>0 },  x</em>1 = x<em>1 - \\eta { \\partial f \\over \\partial x</em>1 } $$</p>\n<ul>\n<li>\\( \\eta \\)(eta)기호는 갱신하는 양을 나타낸다. 보통 0.01이나 0.001 등 특정값으로 정해둔다. 신경망 학습에서는 <strong>학습률</strong>이라고 한다.</li>\n<li>\n<p>위의 식은 1회의 갱신이 일어나는 식이며, 만족스러운 값이 나타날때까지 갱신을 반복한다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># f: 최적화 함수</span>\n<span class=\"token comment\"># init_x: 초기값</span>\n<span class=\"token comment\"># learning_rate: 학습률</span>\n<span class=\"token comment\"># step_num: 경사법에 따른 반복횟수 </span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">gradient_descent</span> <span class=\"token punctuation\">(</span>f<span class=\"token punctuation\">,</span> init_x<span class=\"token punctuation\">,</span> learning_rate<span class=\"token operator\">=</span><span class=\"token number\">0.01</span><span class=\"token punctuation\">,</span> step_num<span class=\"token operator\">=</span><span class=\"token number\">100</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    x<span class=\"token operator\">=</span>init_x\n\n    <span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>step_num<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        grad <span class=\"token operator\">=</span> numerical_gradient<span class=\"token punctuation\">(</span>f<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span>\n        x <span class=\"token operator\">-=</span> lr<span class=\"token operator\">*</span>grad\n    <span class=\"token keyword\">return</span> x</code></pre></div>\n</li>\n<li>학습률과 같은 매개변수를 하이퍼파라미터라고 한다. 가중치/편향과 같은 신경망 매개변수와는 성질이 다른 매개변수이다.</li>\n</ul>","timeToRead":4,"excerpt":"개요 '학습'이란 훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득하는 것을 뜻한다. 매개변수 최적값을 학습할 수 있도록 해주는 지표는 손실함수이다. 왜 정확도가 아닌 손실함수인가? 정확도를 지표료 하면 매개변수의 미분이 대부분의 장소에서…","frontmatter":{"title":"Deep Learning From Scratch - 2. 신경망 학습","cover":"","date":"2019-06-02T14:20:01.000Z","categories":["ML"],"tags":["machine-learning","deep-learning"]},"fields":{"slug":"/deep-learning-from-scratch-2-신경망-학습","date":"June 01, 2019"}}},"pageContext":{"slug":"/deep-learning-from-scratch-2-신경망-학습","nexttitle":"ElasticSearch - unassigned shard 문제 해결 (1) shard가 너무 많을 때","nextslug":"/elastic-search-unassigned-shard-문제-해결-1-shard가-너무-많을-때","prevtitle":"hexo에 mathjax 적용하기","prevslug":"/hexo에-mathjax-적용하기"}}}