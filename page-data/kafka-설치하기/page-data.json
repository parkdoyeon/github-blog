{"componentChunkName":"component---src-templates-post-js","path":"/kafka-설치하기","result":{"data":{"markdownRemark":{"html":"<p>팀에서 관리하는 웹서버의 로깅시스템을 구축하면서 <a href=\"https://kafka.apache.org/\">kafka</a>를 사용하기로 했다. kafka는 서버간 데이터 전달을 위한 브로커 어플리케이션으로, 급격하게 증가하는 데이터를 장애없이 빠르게 보낼수 있는 messaging queue 역할을 한다. kafka를 사용하기로 한 데 있어서 결정적인 요소는 두가지였는데,</p>\n<ol>\n<li>게임 사이트, 런처를 관리하는 우리팀의 경우 서비스는 오픈이나 트래픽을 공격과 같이 급격하게 데이터가 밀려들어오는 이벤트가 자주 발생하고,</li>\n<li>기존 로깅시스템에서 사용되던 <a href=\"https://www.fluentd.org/\">fluntd</a>를 밀어내면서 fluntd의 대용량 처리기능을 보완하는 시스템이 필요했다.</li>\n</ol>\n<p>아직 테스트 단계라 얼만큼 더 효과적으로 사용되고 있는지 확인하긴 어렵지만, 카프카의 취지와 기능을 고려했을때 현재 로깅 시스템 환경에 적합할 것이라고 판단했다.</p>\n<h3 id=\"kafka와-zookeeper\"><a href=\"#kafka%EC%99%80-zookeeper\" aria-label=\"kafka와 zookeeper permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><strong>kafka와 zookeeper</strong></h3>\n<p>kafka는 클러스터링 구성하는데 있어서 분산 시스템을 관리하는(coordination) 기능이 내장되어있지 않고 따로 주키퍼를 사용한다(Apache에서 개발리소스를 생각해서 함께 개발하지 않았다고 한다).</p>\n<p>주키퍼는 카프카 application의 정보를 중앙에 집중하고 구성관리, 그룹관리네이밍, 동기화 등의 서비스를 제공한다. 아파치 카프카 공식 사이트에서 작성시점 현재 최신버전인 1.1.1버전을 받으면 함께 인스톨이 된다.</p>\n<p>주키퍼는 다수가 되는 서버가 살아있으면 죽은 서비스를 살려내는 구조이므로, 홀수단위로 운용한다. 하지만 카프카 브로커를 1개만 운용한다고 해서 주키퍼 실행하지 않아도 되는 것은 아니므로, <u>반드시 카프카와의 연결 설정파일을 작성하고 먼저 실행을 시켜주도록하자.</u></p>\n<h3 id=\"설치방법\"><a href=\"#%EC%84%A4%EC%B9%98%EB%B0%A9%EB%B2%95\" aria-label=\"설치방법 permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><strong>설치방법</strong></h3>\n<ul>\n<li>JDK기반으로 돌아가기때문에 실행환경에서는 꼭 자바를 먼저 설치해놓도록 하자.\n(나의경우 카프카를 도커 이미지로 빌드했기 때문에 카프카는 볼륨으로 연결해서 실행하고, JDK는 이미지에 직접 설치했다)</li>\n<li>\n<p><a href=\"https://kafka.apache.org/downloads\">https://kafka.apache.org/downloads</a> 에서 apache에서 권장하는 미러사이트로 tar파일 설치 </p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">$ <span class=\"token function\">wget</span> http://mirror.navercorp.com/apache/kafka/1.1.1/kafka_2.11-1.1.1.tgz</code></pre></div>\n</li>\n<li>\n<p>압축풀기</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">$ <span class=\"token function\">tar</span> -xvf kafka_2.11-1.1.1.tgz</code></pre></div>\n</li>\n<li>\n<p>압축푼 폴더명 명령어 입력하기 편하도록 변경하기</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">$ <span class=\"token function\">mv</span> kafka_2.11-1.1.1 kafka</code></pre></div>\n</li>\n</ul>\n<h3 id=\"zookeeper-설정파일-변경하기\"><a href=\"#zookeeper-%EC%84%A4%EC%A0%95%ED%8C%8C%EC%9D%BC-%EB%B3%80%EA%B2%BD%ED%95%98%EA%B8%B0\" aria-label=\"zookeeper 설정파일 변경하기 permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><strong>zookeeper 설정파일 변경하기</strong></h3>\n<ul>\n<li>하단에 server.id로 입력된 주키퍼 서버 아이디의 넘버값은 dataDir경로에 myid이라는 파일명으로 작성되어있어야한다.</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">$ <span class=\"token function\">cat</span> kafka/zkdata/myid <span class=\"token comment\"># id가 1인 zookeeper데이터의 id파일</span>\n<span class=\"token number\">1</span>\n$ <span class=\"token builtin class-name\">cd</span> kafka/config\n$ <span class=\"token function\">vi</span> zookeeper.properties</code></pre></div>\n<pre><code class=\"yml\">\ndataDir=/home/user/kafka/zkdata # 주키퍼의 트랜젝션 로그와 스냅샷이 저장되는 저장경로, 직접 편한 경로에 만들어주면 된다.\nclientPort=2181 # client가 연결하는 TCP 포트\nmaxClientCnxns=0 # disable the per-ip limit on the number of connections since this is a non-production config\ninitLimit=5 # 팔로워가 리더가 초기에 연결하는 시간에 대한 타임아웃 tick 수\nsyncLimit=2 # 팔로워가 리더와 동기화하는 시간에 대한 타임아웃 tick수 (주키퍼에 저장된 데이터가 크면 늘려야함)\n\n#서버.서버id=서버ip:리더인 경우 팔로워에 연결할 때 사용하는포트:리더 선출 시점에 사용하는 포트\n#주키퍼 앙상블 전체의 id값과 서버, 포트를 입력해주면 된다. 나의 경우 같은 서버에 3개의 서비스를 올리려고했기 때문에 포트를 다 다르게 넣어줬다. 한 서버 내에서 리더팔로워 포트만 구분되면 된다.\nserver.1=localhost:2888:3888\nserver.2=localhost:2889:3889\nserver.3=localhost:2890:3890\n</code></pre>\n<h3 id=\"kafka-설정파일-변경하기\"><a href=\"#kafka-%EC%84%A4%EC%A0%95%ED%8C%8C%EC%9D%BC-%EB%B3%80%EA%B2%BD%ED%95%98%EA%B8%B0\" aria-label=\"kafka 설정파일 변경하기 permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><strong>kafka 설정파일 변경하기</strong></h3>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">$ <span class=\"token function\">vi</span> server.properties</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"yml\"><pre class=\"language-yml\"><code class=\"language-yml\">broker.id=1 <span class=\"token comment\"># The id of the broker. This must be set to a unique integer for each broker.</span>\nport=9092\ndelete.topic.enable=true <span class=\"token comment\">#없으면 토픽삭제시 삭제가 안되므로 반드시 넣자</span>\n\n<span class=\"token comment\">############################# Socket Server Settings #############################</span>\n\n<span class=\"token comment\"># The address the socket server listens on. It will get the value returned from</span>\n<span class=\"token comment\"># java.net.InetAddress.getCanonicalHostName() if not configured.</span>\n<span class=\"token comment\">#   FORMAT:</span>\n<span class=\"token comment\">#     listeners = listener_name://host_name:port</span>\n<span class=\"token comment\">#   EXAMPLE:</span>\n<span class=\"token comment\">#     listeners = PLAINTEXT://your.host.name:9092</span>\n<span class=\"token comment\"># listeners=PLAINTEXT://:9092</span>\n\n<span class=\"token comment\"># Hostname and port the broker will advertise to producers and consumers. If not set,</span>\n<span class=\"token comment\"># it uses the value for \"listeners\" if configured.  Otherwise, it will use the value</span>\n<span class=\"token comment\"># returned from java.net.InetAddress.getCanonicalHostName().</span>\n<span class=\"token comment\"># 이걸 설정을 안해주면 다른 서버에서 카프카로 통신이 되지 않는다ㅠㅠ</span>\n<span class=\"token comment\"># 심지어 로컬로 다 포트바인딩을 해줬는데도 나는 도커 네트워크를 한번 타서 그랬는지 되지않았음.</span>\nadvertised.listeners=PLAINTEXT<span class=\"token punctuation\">:</span>//카프카서버IP<span class=\"token punctuation\">:</span><span class=\"token number\">9092</span>\n\n<span class=\"token comment\"># Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details</span>\n<span class=\"token comment\">#listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL</span>\n\n<span class=\"token comment\"># The number of threads that the server uses for receiving requests from the network and sending responses to the network</span>\nnum.network.threads=3\n\n<span class=\"token comment\"># The number of threads that the server uses for processing requests, which may include disk I/O</span>\nnum.io.threads=8\n\n<span class=\"token comment\"># The send buffer (SO_SNDBUF) used by the socket server</span>\nsocket.send.buffer.bytes=102400\n\n<span class=\"token comment\"># The receive buffer (SO_RCVBUF) used by the socket server</span>\nsocket.receive.buffer.bytes=102400\n\n<span class=\"token comment\"># The maximum size of a request that the socket server will accept (protection against OOM)</span>\nsocket.request.max.bytes=104857600\n\n<span class=\"token comment\">############################# Log Basics #############################</span>\n\n<span class=\"token comment\"># A comma separated list of directories under which to store log files</span>\nlog.dirs=/home/doyeon/kafka/kfdata\n\n<span class=\"token comment\"># The default number of log partitions per topic. More partitions allow greater</span>\n<span class=\"token comment\"># parallelism for consumption, but this will also result in more files across</span>\n<span class=\"token comment\"># the brokers.</span>\nnum.partitions=1\n\n<span class=\"token comment\"># The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.</span>\n<span class=\"token comment\"># This value is recommended to be increased for installations with data dirs located in RAID array.</span>\nnum.recovery.threads.per.data.dir=1\n\n<span class=\"token comment\">############################# Internal Topic Settings  #############################</span>\n<span class=\"token comment\"># The replication factor for the group metadata internal topics \"__consumer_offsets\" and \"__transaction_state\"</span>\n<span class=\"token comment\"># For anything other than development testing, a value greater than 1 is recommended for to ensure availability such as 3.</span>\noffsets.topic.replication.factor=1\ntransaction.state.log.replication.factor=1\ntransaction.state.log.min.isr=1\n\n<span class=\"token comment\">############################# Log Retention Policy #############################</span>\n\n<span class=\"token comment\"># The following configurations control the disposal of log segments. The policy can</span>\n<span class=\"token comment\"># be set to delete segments after a period of time, or after a given size has accumulated.</span>\n<span class=\"token comment\"># A segment will be deleted whenever *either* of these criteria are met. Deletion always happens</span>\n<span class=\"token comment\"># from the end of the log.</span>\n\n<span class=\"token comment\"># The minimum age of a log file to be eligible for deletion due to age</span>\nlog.retention.hours=168\n\n<span class=\"token comment\"># A size-based retention policy for logs. Segments are pruned from the log unless the remaining</span>\n<span class=\"token comment\"># segments drop below log.retention.bytes. Functions independently of log.retention.hours.</span>\n<span class=\"token comment\">#log.retention.bytes=1073741824</span>\n\n<span class=\"token comment\"># The maximum size of a log segment file. When this size is reached a new log segment will be created.</span>\nlog.segment.bytes=1073741824\n\n<span class=\"token comment\"># The interval at which log segments are checked to see if they can be deleted according</span>\n<span class=\"token comment\"># to the retention policies</span>\nlog.retention.check.interval.ms=300000\n\n<span class=\"token comment\">############################# zookeeper #############################</span>\n\n<span class=\"token comment\"># zookeeper connection string (see zookeeper docs for details).</span>\n<span class=\"token comment\"># This is a comma separated host:port pairs, each corresponding to a zk</span>\n<span class=\"token comment\"># server. e.g. \"127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002\".</span>\n<span class=\"token comment\"># You can also append an optional chroot string to the urls to specify the</span>\n<span class=\"token comment\"># root directory for all kafka znodes.</span>\nzookeeper.connect=localhost<span class=\"token punctuation\">:</span><span class=\"token number\">2181</span><span class=\"token punctuation\">,</span>localhost<span class=\"token punctuation\">:</span><span class=\"token number\">2182</span><span class=\"token punctuation\">,</span>localhost<span class=\"token punctuation\">:</span><span class=\"token number\">2183</span>\n\n<span class=\"token comment\"># Timeout in ms for connecting to zookeeper</span>\nzookeeper.connection.timeout.ms=6000\n\n<span class=\"token comment\">############################# Group Coordinator Settings #############################</span>\n\n<span class=\"token comment\"># The following configuration specifies the time, in milliseconds, that the GroupCoordinator will delay the initial consumer rebalance.</span>\n<span class=\"token comment\"># The rebalance will be further delayed by the value of group.initial.rebalance.delay.ms as new members join the group, up to a maximum of max.poll.interval.ms.</span>\n<span class=\"token comment\"># The default value for this is 3 seconds.</span>\n<span class=\"token comment\"># We override this to 0 here as it makes for a better out-of-the-box experience for development and testing.</span>\n<span class=\"token comment\"># However, in production environments the default value of 3 seconds is more suitable as this will help to avoid unnecessary, and potentially expensive, rebalances during application startup.</span>\ngroup.initial.rebalance.delay.ms=0</code></pre></div>","timeToRead":5,"excerpt":"팀에서 관리하는 웹서버의 로깅시스템을 구축하면서 kafka를 사용하기로 했다. kafka는 서버간 데이터 전달을 위한 브로커 어플리케이션으로, 급격하게 증가하는 데이터를 장애없이 빠르게 보낼수 있는 messaging queue 역할을 한다. kafka…","frontmatter":{"title":"kafka - 설치하기","cover":"","date":"2018-08-11T06:46:00.000Z","categories":["ELK"],"tags":["kafka","zookeeper","logger"]},"fields":{"slug":"/kafka-설치하기","date":"August 10, 2018"}}},"pageContext":{"slug":"/kafka-설치하기","nexttitle":"iOS App을 In House로 배포하기","nextslug":"/i-os-app을-in-house로-배포하기","prevtitle":"c# 기본서 - 객체지향","prevslug":"/c-기본서-객체지향"}}}